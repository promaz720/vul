import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import json
import os
from datetime import datetime
import tempfile
import zipfile
import io
import shutil
from pathlib import Path


def sanitize_filename(name: str) -> str:
    return "".join(c for c in name if c.isalnum() or c in (' ', '-', '_')).rstrip().replace(' ', '_')

class WebsiteVulnerabilityScanner:
    def __init__(self):
        self.findings = []
        self.visited_urls = set()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def scan_website(self, base_url, max_pages=10):
        """Main scanning function"""
        self.findings = []
        self.visited_urls = set()

        st.info(f"üîç Starting comprehensive scan of {base_url}")

        try:
            # Basic information gathering
            self.gather_basic_info(base_url)

            # Crawl website
            urls_to_scan = self.crawl_website(base_url, max_pages)

            # Scan each page for vulnerabilities
            for url in urls_to_scan:
                self.scan_single_page(url)

            # Generate HackerOne report
            self.generate_hackerone_report()

        except Exception as e:
            st.error(f"Error during scanning: {str(e)}")

    def gather_basic_info(self, url):
        """Gather basic website information"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')

            # Server information
            server = response.headers.get('Server', 'Unknown')
            x_powered_by = response.headers.get('X-Powered-By', 'Not disclosed')

            # Technology detection
            technologies = self.detect_technologies(soup, response.headers)

            # Basic security headers check
            security_headers = self.check_security_headers(response.headers, url)

            self.findings.append({
                'type': 'info',
                'category': 'Website Information',
                'title': 'Basic Website Information',
                'description': f"""
**URL:** {url}
**Status Code:** {response.status_code}
**Server:** {server}
**X-Powered-By:** {x_powered_by}
**Content Length:** {len(response.content)} bytes
**Technologies Detected:** {', '.join(technologies)}
**Security Headers:** {len(security_headers)} found
                """.strip(),
                'severity': 'info',
                'cvss': 'N/A',
                'remediation': 'Informational finding'
            })

        except Exception as e:
            st.error(f"Error gathering basic info: {str(e)}")

    def detect_technologies(self, soup, headers):
        """Detect technologies used by the website"""
        technologies = []

        # Check for common frameworks and libraries
        scripts = soup.find_all('script', src=True)
        meta_tags = soup.find_all('meta')

        for script in scripts:
            src = script.get('src', '').lower()
            if 'jquery' in src:
                technologies.append('jQuery')
            elif 'bootstrap' in src:
                technologies.append('Bootstrap')
            elif 'react' in src:
                technologies.append('React')
            elif 'angular' in src:
                technologies.append('Angular')
            elif 'vue' in src:
                technologies.append('Vue.js')

        # Check meta tags
        for meta in meta_tags:
            if meta.get('name') == 'generator':
                generator = meta.get('content', '').lower()
                if 'wordpress' in generator:
                    technologies.append('WordPress')
                elif 'drupal' in generator:
                    technologies.append('Drupal')
                elif 'joomla' in generator:
                    technologies.append('Joomla')

        return list(set(technologies))

    def check_security_headers(self, headers, url):
        """Check for security headers"""
        security_headers = {}
        required_headers = {
            'Content-Security-Policy': 'Missing CSP header',
            'X-Frame-Options': 'Missing X-Frame-Options header',
            'X-Content-Type-Options': 'Missing X-Content-Type-Options header',
            'Strict-Transport-Security': 'Missing HSTS header',
            'X-XSS-Protection': 'Missing X-XSS-Protection header',
            'Referrer-Policy': 'Missing Referrer-Policy header'
        }

        for header, description in required_headers.items():
            if header in headers:
                security_headers[header] = headers[header]
            else:
                # Provide detailed information for each missing header
                header_details = {
                    'Content-Security-Policy': {
                        'description': f"""**Vulnerability Details:**
The website is missing the {header} security header which helps prevent various client-side attacks including Cross-Site Scripting (XSS).

**Exploitation Method:**
1. Attacker can inject malicious scripts into the website
2. Users visiting the site may execute attacker-controlled code
3. Can lead to session hijacking, data theft, or malware distribution

**Proof of Concept:**
Without CSP, an attacker can:
```html
<script>alert('XSS')</script>
<img src=x onerror=alert('XSS')>
```

**Impact:**
- Session hijacking and cookie theft
- Keylogging and credential theft
- Cryptocurrency mining
- Website defacement""".strip(),
                        'remediation': f'Add {header} header: Content-Security-Policy: default-src \'self\'; script-src \'self\' \'unsafe-inline\'; style-src \'self\' \'unsafe-inline\''
                    },
                    'X-Frame-Options': {
                        'description': f"""**Vulnerability Details:**
Missing {header} allows attackers to embed this website in iframes on malicious sites (Clickjacking).

**Exploitation Method:**
1. Attacker creates a malicious page with transparent iframe
2. Overlays legitimate site with fake UI elements
3. Tricks users into clicking attacker-controlled actions

**Proof of Concept:**
```html
<iframe src="{url}" style="opacity: 0.1;"></iframe>
<div style="position: absolute; top: 100px; left: 50px;">
    <button>Click me (actually clicking iframe content)</button>
</div>
```

**Impact:**
- Unauthorized actions performed as victim
- Credential theft through fake login forms
- Likejacking and content manipulation""".strip(),
                        'remediation': f'Add {header} header: X-Frame-Options: SAMEORIGIN or DENY'
                    },
                    'Strict-Transport-Security': {
                        'description': f"""**Vulnerability Details:**
Missing {header} makes the site vulnerable to protocol downgrade attacks and MITM attacks.

**Exploitation Method:**
1. Attacker performs SSL stripping attack
2. Forces user connection over unencrypted HTTP
3. Intercepts and modifies traffic

**Impact:**
- Credential theft
- Session hijacking
- Data interception""".strip(),
                        'remediation': f'Add {header} header: Strict-Transport-Security: max-age=31536000; includeSubDomains'
                    }
                }

                detail = header_details.get(header, {'description': description, 'remediation': f'Add the {header} header to improve security'})

                self.findings.append({
                    'type': 'security_headers',
                    'category': 'Security Headers',
                    'title': f'Missing {header}',
                    'description': detail['description'],
                    'severity': 'low',
                    'cvss': '2.0',
                    'remediation': detail['remediation'],
                    'exploitation_method': f'{header} bypass attack',
                    'references': 'OWASP Security Headers Project'
                })

        return security_headers

    def crawl_website(self, base_url, max_pages):
        """Crawl website to find all pages"""
        urls_to_scan = [base_url]
        self.visited_urls.add(base_url)

        try:
            response = self.session.get(base_url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')

            # Extract links
            links = soup.find_all('a', href=True)
            for link in links:
                href = link['href']
                full_url = urljoin(base_url, href)

                # Only scan same domain
                if urlparse(full_url).netloc == urlparse(base_url).netloc:
                    if full_url not in self.visited_urls and len(urls_to_scan) < max_pages:
                        urls_to_scan.append(full_url)
                        self.visited_urls.add(full_url)

        except Exception as e:
            st.error(f"Error crawling website: {str(e)}")

        return urls_to_scan

    def scan_single_page(self, url):
        """Scan a single page for vulnerabilities"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')

            # Check for various vulnerabilities
            self.check_xss_vulnerabilities(soup, url)
            self.check_sql_injection(soup, url)
            self.check_csrf_vulnerabilities(soup, url)
            self.check_sensitive_data_exposure(soup, url)
            self.check_broken_authentication(soup, url)
            self.check_security_misconfiguration(soup, url)
            self.check_broken_access_control(soup, url)

        except Exception as e:
            st.error(f"Error scanning page {url}: {str(e)}")

    def check_xss_vulnerabilities(self, soup, url):
        """Check for XSS vulnerabilities"""
        forms = soup.find_all('form')

        for form in forms:
            action = form.get('action', '')
            method = form.get('method', 'get').lower()

            # Check if form has CSRF protection
            has_csrf_token = False
            inputs = form.find_all('input')
            for input_field in inputs:
                if input_field.get('type') == 'hidden' and ('csrf' in input_field.get('name', '').lower() or 'token' in input_field.get('name', '').lower()):
                    has_csrf_token = True
                    break

            if not has_csrf_token:
                self.findings.append({
                    'type': 'xss',
                    'category': 'Cross-Site Scripting (XSS)',
                    'title': 'Form without CSRF Protection',
                    'description': f"""
**URL:** {url}
**Form Action:** {action}
**Method:** {method}

**Vulnerability Details:**
This form lacks Cross-Site Request Forgery (CSRF) protection, making it vulnerable to attacks where malicious websites can force users to perform unwanted actions on this site while authenticated.

**Exploitation Method:**
1. Attacker creates a malicious webpage with hidden form fields
2. User is tricked into visiting the attacker's page while logged into the target site
3. Attacker's page automatically submits form to {action} with malicious data
4. Target site processes the request as if it came from the legitimate user

**Proof of Concept:**
```html
<!-- Attacker's malicious page -->
<form action="{urljoin(url, action)}" method="{method}" style="display:none;">
    <input type="hidden" name="malicious_field" value="attacker_controlled_data">
    <!-- Add other required fields -->
</form>
<script>document.forms[0].submit();</script>
```

**Impact:**
- Unauthorized actions performed as the victim user
- Data modification or deletion
- Account compromise
- Reputation damage

**References:**
- OWASP CSRF Prevention Cheat Sheet
- OWASP Top 10: Broken Access Control
                    """.strip(),
                    'severity': 'medium',
                    'cvss': '6.5',
                    'remediation': 'Add CSRF tokens to all forms. Implement same-site cookie attributes and origin header checks.',
                    'exploitation_method': 'CSRF attack via malicious webpage form submission',
                    'poc': 'Available in description above'
                })

    def check_sql_injection(self, soup, url):
        """Check for SQL injection vulnerabilities"""
        forms = soup.find_all('form')

        for form in forms:
            inputs = form.find_all('input')
            for input_field in inputs:
                input_name = input_field.get('name', '')
                input_type = input_field.get('type', '')

                # Look for suspicious input patterns
                if input_name and any(keyword in input_name.lower() for keyword in ['id', 'user', 'search', 'query']):
                    self.findings.append({
                        'type': 'sqli',
                        'category': 'SQL Injection',
                        'title': 'Potential SQL Injection Vector',
                        'description': f"""
**URL:** {url}
**Input Name:** {input_name}
**Input Type:** {input_type}

**Vulnerability Details:**
This input field appears to be used for database queries and may be vulnerable to SQL injection if user input is not properly sanitized or parameterized.

**Exploitation Method:**
1. Identify input fields that interact with databases (search, login, user input)
2. Test with malicious SQL payloads to bypass authentication or extract data
3. Use time-based or error-based techniques to extract database information

**Common SQL Injection Payloads:**
```sql
' OR '1'='1
' OR '1'='1' --
' UNION SELECT username, password FROM users --
'; DROP TABLE users; --
' AND (SELECT COUNT(*) FROM users) > 0 --
```

**Proof of Concept:**
```bash
# Basic test
curl -X POST "{url}" -d "{input_name}=test' OR '1'='1"

# Extract database version
curl -X POST "{url}" -d "{input_name}=test' UNION SELECT @@VERSION --"

# Time-based blind injection
curl -X POST "{url}" -d "{input_name}=test' AND (SELECT SLEEP(5)) --"
```

**Impact:**
- Unauthorized database access
- Data extraction (users, passwords, sensitive information)
- Database modification or deletion
- Server compromise through advanced techniques

**References:**
- OWASP SQL Injection Prevention Cheat Sheet
- OWASP Top 10: Injection
                        """.strip(),
                        'severity': 'high',
                        'cvss': '8.5',
                        'remediation': 'Use prepared statements, stored procedures, or ORM frameworks. Implement input validation and least privilege database accounts.',
                        'exploitation_method': 'SQL injection via malicious input payloads',
                        'poc': 'Available in description above'
                    })

    def check_csrf_vulnerabilities(self, soup, url):
        """Check for CSRF vulnerabilities"""
        # Already covered in XSS check, but add additional checks
        pass

    def check_sensitive_data_exposure(self, soup, url):
        """Check for sensitive data exposure"""
        # Check for email addresses in HTML
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, str(soup))

        if emails:
            self.findings.append({
                'type': 'info_leak',
                'category': 'Information Disclosure',
                'title': 'Email Addresses Exposed',
                'description': f"""
**URL:** {url}
**Exposed Emails:** {', '.join(set(emails))}

**Vulnerability Details:**
Email addresses are exposed in the HTML source code, making them easily harvestable by attackers and bots.

**Exploitation Method:**
1. Automated tools scan websites for email patterns using regex
2. Attackers build databases of email addresses for spam campaigns
3. Social engineering attacks target individuals using harvested emails
4. Email addresses used for account enumeration and password attacks

**Proof of Concept:**
```bash
# Simple email harvesting script
import requests
import re

response = requests.get('{url}')
emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{{2,}}\\b', response.text)
print('Found emails:', emails)
```

**Impact:**
- Spam and phishing campaigns targeting exposed emails
- Social engineering attacks using personal information
- Account enumeration for targeted attacks
- Reputation damage from spam complaints

**References:**
- OWASP Information Disclosure
- Personal Data Protection Guidelines
                """.strip(),
                'severity': 'low',
                'cvss': '2.0',
                'remediation': 'Remove email addresses from client-side code. Use contact forms or obfuscate emails with JavaScript or CSS.',
                'exploitation_method': 'Automated email harvesting and social engineering',
                'poc': 'Available in description above'
            })

        # Check for other sensitive patterns
        sensitive_patterns = {
            'Phone Numbers': r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
            'Social Security Numbers': r'\b\d{3}-\d{2}-\d{4}\b',
            'Credit Card Numbers': r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b',
            'API Keys': r'\b[A-Za-z0-9]{32,}\b',  # Generic pattern for API keys
        }

        for pattern_name, pattern in sensitive_patterns.items():
            matches = re.findall(pattern, str(soup))
            if matches:
                self.findings.append({
                    'type': 'info_leak',
                    'category': 'Information Disclosure',
                    'title': f'{pattern_name} Exposed',
                    'description': f"""
**URL:** {url}
**Pattern:** {pattern_name}
**Found:** {len(matches)} potential matches

**Vulnerability Details:**
Potential {pattern_name.lower()} found in HTML source code.

**Impact:**
- Identity theft and fraud
- Unauthorized access to services
- Financial loss
- Privacy violations

**Remediation:**
Remove sensitive data from client-side code and implement proper data handling procedures.
                    """.strip(),
                    'severity': 'medium',
                    'cvss': '4.0',
                    'remediation': f'Remove {pattern_name.lower()} from client-side code. Store sensitive data server-side only.',
                    'exploitation_method': f'Automated harvesting of {pattern_name.lower()}',
                    'references': 'OWASP Sensitive Data Exposure'
                })

    def check_broken_authentication(self, soup, url):
        """Check for broken authentication"""
        # Look for weak password policies
        password_inputs = soup.find_all('input', {'type': 'password'})

        for pwd_input in password_inputs:
            minlength = pwd_input.get('minlength', '')
            pattern = pwd_input.get('pattern', '')

            if not minlength or int(minlength) < 8:
                self.findings.append({
                    'type': 'auth',
                    'category': 'Broken Authentication',
                    'title': 'Weak Password Policy',
                    'description': f"""
**URL:** {url}
**Minimum Length:** {minlength or 'Not set'}
**Pattern:** {pattern or 'Not set'}

Password field does not enforce strong password requirements.
                    """.strip(),
                    'severity': 'medium',
                    'cvss': '5.0',
                    'remediation': 'Enforce strong password policies (minimum 8 characters, complexity requirements)'
                })

    def check_security_misconfiguration(self, soup, url):
        """Check for security misconfiguration"""
        # Check for default pages
        default_files = ['admin.php', 'admin.aspx', 'login.php', 'config.php', 'backup.sql']
        current_path = urlparse(url).path

        for default_file in default_files:
            if default_file in current_path:
                self.findings.append({
                    'type': 'misconfig',
                    'category': 'Security Misconfiguration',
                    'title': 'Potential Default File Access',
                    'description': f"""
**URL:** {url}

Accessing what appears to be a default administrative or configuration file.
                    """.strip(),
                    'severity': 'medium',
                    'cvss': '5.0',
                    'remediation': 'Remove default files from production or restrict access with proper authentication'
                })

    def check_broken_access_control(self, soup, url):
        """Check for broken access control"""
        # Look for admin panels without authentication
        admin_keywords = ['admin', 'administrator', 'moderator', 'dashboard', 'control-panel']

        if any(keyword in url.lower() for keyword in admin_keywords):
            # Check if page requires authentication
            forms = soup.find_all('form')
            has_login_form = any('login' in str(form).lower() or 'password' in str(form).lower() for form in forms)

            if not has_login_form:
                self.findings.append({
                    'type': 'access_control',
                    'category': 'Broken Access Control',
                    'title': 'Administrative Page Without Authentication',
                    'description': f"""
**URL:** {url}

Administrative page accessible without apparent authentication requirements.
                    """.strip(),
                    'severity': 'high',
                    'cvss': '7.5',
                    'remediation': 'Implement proper authentication and authorization for administrative pages'
                })

    def generate_hackerone_report(self):
        """Generate HackerOne formatted report"""
        if not self.findings:
            st.success("‚úÖ No vulnerabilities found!")
            return

        # Consolidate duplicate/repeated findings into aggregated findings
        self._consolidate_findings()

        # Group findings by severity
        severity_count = {'high': 0, 'medium': 0, 'low': 0, 'info': 0}
        for finding in self.findings:
            severity_count[finding['severity']] += 1
        # Normalize findings by adding an explicit 'proof' field extracted from known keys
        normalized_findings = []
        for f in self.findings:
            # shallow copy to avoid mutating original
            nf = dict(f)
            nf['proof'] = self._extract_proof_from_finding(nf)
            normalized_findings.append(nf)

        # Create report
        report = {
            'title': f'Website Security Assessment - {len(self.findings)} findings',
            'summary': f"""
Comprehensive security scan completed with the following findings:
- High severity: {severity_count['high']}
- Medium severity: {severity_count['medium']}
- Low severity: {severity_count['low']}
- Informational: {severity_count['info']}

Scan performed on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            """.strip(),
            'findings': normalized_findings,
            'recommendations': self.generate_recommendations()
        }

        return report

    def _extract_proof_from_finding(self, finding):
        """Try to extract a concise Proof of Concept from a finding.

        Sources checked (in order): explicit 'proof' key, 'poc', 'exploitation_method',
        'description' code blocks under 'Proof of Concept', or fallback to the exploitation_method.
        Returns a string or None.
        """
        # explicit keys
        for key in ('proof', 'poc', 'exploitation_method'):
            if key in finding and finding[key]:
                return finding[key]

        # try to extract code block from description
        desc = finding.get('description', '')
        # look for fenced code blocks
        code_block = re.search(r"```(.*?)```", desc, re.S)
        if code_block:
            return code_block.group(0).strip()

        # look for a "Proof of Concept:" label and capture until next blank line or next heading
        poc_match = re.search(r"Proof of Concept:\s*(.*?)(?:\n\n|\n\*\*Impact\*\*|\n\*\*References\*\*|$)", desc, re.S | re.I)
        if poc_match:
            poc_text = poc_match.group(1).strip()
            if poc_text:
                return poc_text

        return None

    def _consolidate_findings(self):
        """Merge duplicate findings into aggregated findings.

        Duplicates are determined by (type, category, title) lowercased. For each group,
        build an aggregated finding that includes a count and an 'occurrences' list with URLs or snippets.
        """
        grouped = {}
        for f in self.findings:
            key = (f.get('type','').lower(), f.get('category','').lower(), f.get('title','').lower())
            if key not in grouped:
                grouped[key] = []
            grouped[key].append(f)

        consolidated = []
        for key, items in grouped.items():
            if len(items) == 1:
                # keep original
                consolidated.append(items[0])
                continue

            # merge
            base = dict(items[0])
            occurrences = []
            for it in items:
                # try to extract a URL or identifying text from description
                url_match = None
                try:
                    import re
                    m = re.search(r"\*\*URL:\*\*\s*(https?://[^\s]+)", it.get('description',''))
                    if m:
                        url_match = m.group(1)
                except Exception:
                    url_match = None

                # fallback: use any URL-like substring or the entire title
                if not url_match:
                    try:
                        m2 = re.search(r"https?://[\w\-\./?=&%]+", it.get('description',''))
                        if m2:
                            url_match = m2.group(0)
                    except Exception:
                        url_match = None

                if not url_match:
                    # sometimes the finding includes an action/path in description
                    url_match = it.get('description','').splitlines()[0][:200]

                occurrences.append({'url': url_match, 'raw': it.get('description','')})

            base['occurrences'] = occurrences
            base['count'] = len(items)
            # update title to indicate aggregation
            base['title'] = f"{base.get('title')} (occurred {base['count']} times)"
            consolidated.append(base)

        self.findings = consolidated

    def generate_recommendations(self):
        """Generate general recommendations"""
        recommendations = [
            "Implement proper input validation and sanitization",
            "Use prepared statements for database queries",
            "Implement CSRF protection on all forms",
            "Use HTTPS for all communications",
            "Keep software and dependencies updated",
            "Implement proper session management",
            "Use security headers (CSP, HSTS, etc.)",
            "Regular security audits and penetration testing"
        ]

        return recommendations


def parse_markdown_report(md_text: str) -> dict:
    """Rudimentary parser converting the exported HackerOne markdown into the report dict shape.

    This looks for headings like '### {title}' followed by '- **Category:** ...', 'Description:', 'Remediation:', and optional 'Proof of Concept:' and 'Occurrences:'.
    It returns a dict with keys: title, summary, findings (list), recommendations.
    """
    lines = md_text.splitlines()
    report = {'title': 'Imported Report', 'summary': '', 'findings': [], 'recommendations': []}
    i = 0
    current = None
    while i < len(lines):
        line = lines[i].strip()
        if line.startswith('# '):
            report['title'] = line[2:].strip()
        elif line.startswith('## Summary'):
            i += 1
            # read until next heading
            summary_lines = []
            while i < len(lines) and not lines[i].startswith('## '):
                summary_lines.append(lines[i])
                i += 1
            report['summary'] = '\n'.join(summary_lines).strip()
            continue
        elif line.startswith('### '):
            # new finding
            if current:
                report['findings'].append(current)
            title = line[4:].strip()
            current = {'title': title, 'category': '', 'description': '', 'remediation': '', 'severity': 'info', 'cvss': 'N/A'}
            i += 1
            continue
        elif current is not None:
            # inside a finding, look for known markers
            if line.startswith('- **Category:**'):
                current['category'] = line.split(':',1)[1].strip()
            elif line.startswith('- **Severity:**'):
                current['severity'] = line.split(':',1)[1].strip().lower()
            elif line.startswith('- **CVSS Score:**'):
                current['cvss'] = line.split(':',1)[1].strip()
            elif line.startswith('**Description:**'):
                # read description until next bold heading or 'Proof of Concept' or 'Remediation'
                i += 1
                desc_lines = []
                while i < len(lines) and not lines[i].strip().startswith('**Remediation:**') and not lines[i].strip().startswith('**Proof of Concept:') and not lines[i].strip().startswith('**') and not lines[i].startswith('###'):
                    desc_lines.append(lines[i])
                    i += 1
                current['description'] = '\n'.join(desc_lines).strip()
                continue
            elif line.startswith('**Remediation:**'):
                i += 1
                rem_lines = []
                while i < len(lines) and not lines[i].strip().startswith('**Proof of Concept:') and not lines[i].startswith('###'):
                    rem_lines.append(lines[i])
                    i += 1
                current['remediation'] = '\n'.join(rem_lines).strip()
                continue
            elif line.startswith('**Proof of Concept:**'):
                i += 1
                poc_lines = []
                # capture fenced or plain until separator
                while i < len(lines) and not lines[i].startswith('---') and not lines[i].startswith('###'):
                    poc_lines.append(lines[i])
                    i += 1
                current['poc'] = '\n'.join(poc_lines).strip()
                continue
        i += 1

    if current:
        report['findings'].append(current)

    # recommendations: try to capture trailing section
    if '\n## General Recommendations' in md_text:
        parts = md_text.split('\n## General Recommendations')
        rec_text = parts[-1]
        report['recommendations'] = [line.strip('- ').strip() for line in rec_text.splitlines() if line.strip().startswith('-')]

    return report


def _generate_validation_zip(report: dict) -> bytes:
    """Create validation artifacts in a temp dir and return a zip file's bytes."""
    base = Path(tempfile.mkdtemp())
    out_dir = base / f"validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    out_dir.mkdir()

    # ensure proof fields
    scanner = WebsiteVulnerabilityScanner()
    for i, f in enumerate(report.get('findings', []), 1):
        if 'proof' not in f:
            f['proof'] = scanner._extract_proof_from_finding(f) if hasattr(scanner, '_extract_proof_from_finding') else f.get('poc')
        # write markdown file
        md_name = out_dir / (f"{i:02d}_" + sanitize_filename(f.get('title','finding')) + '.md')
        with md_name.open('w', encoding='utf-8') as fh:
            fh.write(f"# {f.get('title')}\n\n")
            fh.write(f"**Severity:** {f.get('severity','info').upper()}\n\n")
            fh.write(f"**Category:** {f.get('category','')}\n\n")
            fh.write("## Description\n\n")
            fh.write(f"{f.get('description','')}\n\n")
            if f.get('proof'):
                fh.write("## Proof of Concept\n\n```")
                fh.write(str(f.get('proof')))
                fh.write("\n```\n\n")
            fh.write("## Remediation\n\n")
            fh.write(f"{f.get('remediation','')}\n")

        # small PoC artifacts similar to validation script
        if f.get('type') in ('xss',) or 'csrf' in f.get('category','').lower() or 'csrf' in f.get('title','').lower():
            html_name = out_dir / (f"{i:02d}_" + sanitize_filename(f.get('title','')) + '.html')
            with html_name.open('w', encoding='utf-8') as fh:
                poc = f.get('proof') or f.get('poc') or ''
                if isinstance(poc, str) and poc.strip().startswith('```'):
                    poc = '\n'.join(line for line in poc.splitlines() if not line.strip().startswith('```'))
                fh.write('<!doctype html>\n<html><body>')
                fh.write(poc)
                fh.write('</body></html>')

        if f.get('type') in ('sqli',) or 'sql injection' in f.get('category','').lower():
            sh_name = out_dir / (f"{i:02d}_" + sanitize_filename(f.get('title','')) + '_poc.sh')
            with sh_name.open('w', encoding='utf-8') as fh:
                fh.write('#!/bin/sh\n')
                fh.write('# Replace TARGET_URL and PARAM\n')
                fh.write('curl -X POST "TARGET_URL" -d "PARAM=\' OR \'1\'=\'1\'"\n')

    # create zip
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, 'w', zipfile.ZIP_DEFLATED) as zf:
        for p in out_dir.rglob('*'):
            zf.write(p, p.relative_to(out_dir))

    # cleanup
    try:
        shutil.rmtree(base)
    except Exception:
        pass

    buf.seek(0)
    return buf.read()

    def generate_recommendations(self):
        """Generate general recommendations"""
        recommendations = [
            "Implement proper input validation and sanitization",
            "Use prepared statements for database queries",
            "Implement CSRF protection on all forms",
            "Use HTTPS for all communications",
            "Keep software and dependencies updated",
            "Implement proper session management",
            "Use security headers (CSP, HSTS, etc.)",
            "Regular security audits and penetration testing"
        ]

        return recommendations

def main():
    st.set_page_config(
        page_title="Website Vulnerability Scanner",
        page_icon="üîí",
        layout="wide"
    )

    st.title("üîí Website Vulnerability Scanner")
    st.markdown("### Comprehensive Bug Bounty Scanner for HackerOne")

    # Sidebar
    st.sidebar.header("Scan Configuration")
    max_pages = st.sidebar.slider("Max Pages to Scan", min_value=1, max_value=50, value=10)
    scan_forms = st.sidebar.checkbox("Scan Forms", value=True)
    scan_headers = st.sidebar.checkbox("Check Security Headers", value=True)

    # Main content
    url = st.text_input("Enter Website URL", placeholder="https://example.com")

    if st.button("üöÄ Start Scan", type="primary"):
        if not url:
            st.error("Please enter a URL to scan")
            return

        # Validate URL
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        # Initialize scanner
        scanner = WebsiteVulnerabilityScanner()

        # Progress bar
        progress_bar = st.progress(0)
        status_text = st.empty()

        # Perform scan
        with st.container():
            scanner.scan_website(url, max_pages)

        progress_bar.progress(100)
        status_text.text("‚úÖ Scan completed!")

        # Display results
        if scanner.findings:
            report = scanner.generate_hackerone_report()

            # Validation file generation UI
            st.header("üßæ Generate Validation Files")

            col_a, col_b = st.columns([2, 1])
            with col_a:
                uploaded = st.file_uploader("Upload HackerOne markdown or JSON report", type=['md', 'markdown', 'json'])
            with col_b:
                use_current = st.button("Use current scan results")

            selected_report = None
            # prioritize uploaded file
            if uploaded is not None:
                raw = uploaded.read().decode('utf-8')
                if uploaded.name.lower().endswith('.json'):
                    try:
                        selected_report = json.loads(raw)
                    except Exception as e:
                        st.error(f"Failed to parse JSON report: {e}")
                else:
                    # parse markdown into report structure
                    try:
                        selected_report = parse_markdown_report(raw)
                    except Exception as e:
                        st.error(f"Failed to parse Markdown report: {e}")
            elif use_current:
                selected_report = report

            if selected_report is not None:
                st.write("Report loaded. Generating validation ZIP...")
                try:
                    zip_bytes = _generate_validation_zip(selected_report)
                    st.download_button(label="üì• Download Validation ZIP", data=zip_bytes, file_name=f"validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip", mime='application/zip')
                except Exception as e:
                    st.error(f"Failed to generate validation files: {e}")

            st.success(f"‚úÖ Scan completed! Found {len(scanner.findings)} findings")

            # Summary
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("High", len([f for f in scanner.findings if f['severity'] == 'high']))
            with col2:
                st.metric("Medium", len([f for f in scanner.findings if f['severity'] == 'medium']))
            with col3:
                st.metric("Low", len([f for f in scanner.findings if f['severity'] == 'low']))
            with col4:
                st.metric("Info", len([f for f in scanner.findings if f['severity'] == 'info']))

            # Detailed findings grouped by severity
            st.header("üìã Detailed Findings")

            grouped = {'high': [], 'medium': [], 'low': [], 'info': []}
            # ensure report findings have 'proof' (report was normalized)
            for f in report['findings']:
                sev = f.get('severity', 'info')
                grouped.setdefault(sev, []).append(f)

            for sev_label in ('high', 'medium', 'low', 'info'):
                items = grouped.get(sev_label, [])
                if not items:
                    continue

                st.subheader(f"{sev_label.capitalize()} Severity ({len(items)})")
                for i, finding in enumerate(items, 1):
                    with st.expander(f"{i}. {finding.get('title', 'Untitled')} ({finding.get('severity','').upper()})"):
                        st.markdown("**Category:** " + finding.get('category', ''))
                        st.markdown("**Description:**")
                        st.markdown(finding.get('description', ''))
                        # Show Proof of Concept separately if available
                        proof = finding.get('proof')
                        if proof:
                            st.markdown("**Proof of Concept:**")
                            # If it's a code block, render as-is; otherwise render inside a code block for clarity
                            if proof.startswith('```'):
                                st.markdown(proof)
                            else:
                                st.code(proof)

                        st.markdown("**CVSS Score:** " + finding.get('cvss', 'N/A'))
                        # Show aggregated occurrences if present
                        if finding.get('count', 1) > 1 or finding.get('occurrences'):
                            st.markdown(f"**Occurrences:** {finding.get('count', len(finding.get('occurrences', [])))} detected")
                            for occ in finding.get('occurrences', []):
                                # display the URL if available
                                occ_url = occ.get('url') or occ.get('raw')[:200]
                                st.write(f"- {occ_url}")
                        st.markdown("**Remediation:**")
                        st.info(finding.get('remediation', ''))

            # HackerOne Export
            st.header("üìÑ HackerOne Report Format")

            report_text = f"""
# {report['title']}

## Summary
{report['summary']}

## Findings
"""

            for finding in report['findings']:
                report_text += f"""
### {finding['title']}
- **Category:** {finding['category']}
- **Severity:** {finding['severity'].upper()}
- **CVSS Score:** {finding['cvss']}

**Description:**
{finding['description']}

**Remediation:**
{finding['remediation']}
"""
                # Include proof if present
                proof = finding.get('proof')
                if proof:
                    report_text += "\n**Proof of Concept:**\n"
                    # If proof is a fenced code block, include as-is, otherwise wrap in triple backticks
                    if proof.startswith('```'):
                        report_text += f"\n{proof}\n"
                    else:
                        report_text += f"\n```\n{proof}\n```\n"

                # Include occurrences if present
                occs = finding.get('occurrences')
                if occs:
                    report_text += "\n**Occurrences:**\n"
                    for o in occs:
                        report_text += f"- {o.get('url') or o.get('raw','')[:200]}\n"

                report_text += "\n---\n"

            report_text += "\n## General Recommendations\n" + "\n".join(f"- {rec}" for rec in report['recommendations'])

            st.download_button(
                label="üì• Download HackerOne Report",
                data=report_text,
                file_name=f"hackerone_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                mime="text/markdown"
            )

            st.text_area("Report Preview", report_text, height=300)
        else:
            st.success("‚úÖ No vulnerabilities found! The website appears to be secure.")

if __name__ == "__main__":
    main()
