import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import json
import os
from datetime import datetime

class WebsiteVulnerabilityScanner:
    def __init__(self):
        self.findings = []
        self.visited_urls = set()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def scan_website(self, base_url, max_pages=10):
        """Main scanning function"""
        self.findings = []
        self.visited_urls = set()

        st.info(f"üîç Starting comprehensive scan of {base_url}")

        try:
            # Basic information gathering
            self.gather_basic_info(base_url)

            # Crawl website
            urls_to_scan = self.crawl_website(base_url, max_pages)

            # Scan each page for vulnerabilities
            for url in urls_to_scan:
                self.scan_single_page(url)

            # Generate HackerOne report
            self.generate_hackerone_report()

        except Exception as e:
            st.error(f"Error during scanning: {str(e)}")

    def gather_basic_info(self, url):
        """Gather basic website information"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')

            # Server information
            server = response.headers.get('Server', 'Unknown')
            x_powered_by = response.headers.get('X-Powered-By', 'Not disclosed')

            # Technology detection
            technologies = self.detect_technologies(soup, response.headers)

            # Basic security headers check
            security_headers = self.check_security_headers(response.headers, url)

            self.findings.append({
                'type': 'info',
                'category': 'Website Information',
                'title': 'Basic Website Information',
                'description': f"""
**URL:** {url}
**Status Code:** {response.status_code}
**Server:** {server}
**X-Powered-By:** {x_powered_by}
**Content Length:** {len(response.content)} bytes
**Technologies Detected:** {', '.join(technologies)}
**Security Headers:** {len(security_headers)} found
                """.strip(),
                'severity': 'info',
                'cvss': 'N/A',
                'remediation': 'Informational finding'
            })

        except Exception as e:
            st.error(f"Error gathering basic info: {str(e)}")

    def detect_technologies(self, soup, headers):
        """Detect technologies used by the website"""
        technologies = []

        # Check for common frameworks and libraries
        scripts = soup.find_all('script', src=True)
        meta_tags = soup.find_all('meta')

        for script in scripts:
            src = script.get('src', '').lower()
            if 'jquery' in src:
                technologies.append('jQuery')
            elif 'bootstrap' in src:
                technologies.append('Bootstrap')
            elif 'react' in src:
                technologies.append('React')
            elif 'angular' in src:
                technologies.append('Angular')
            elif 'vue' in src:
                technologies.append('Vue.js')

        # Check meta tags
        for meta in meta_tags:
            if meta.get('name') == 'generator':
                generator = meta.get('content', '').lower()
                if 'wordpress' in generator:
                    technologies.append('WordPress')
                elif 'drupal' in generator:
                    technologies.append('Drupal')
                elif 'joomla' in generator:
                    technologies.append('Joomla')

        return list(set(technologies))

    def check_security_headers(self, headers, url):
        """Check for security headers"""
        security_headers = {}
        required_headers = {
            'Content-Security-Policy': 'Missing CSP header',
            'X-Frame-Options': 'Missing X-Frame-Options header',
            'X-Content-Type-Options': 'Missing X-Content-Type-Options header',
            'Strict-Transport-Security': 'Missing HSTS header',
            'X-XSS-Protection': 'Missing X-XSS-Protection header',
            'Referrer-Policy': 'Missing Referrer-Policy header'
        }

        for header, description in required_headers.items():
            if header in headers:
                security_headers[header] = headers[header]
            else:
                # Provide detailed information for each missing header
                header_details = {
                    'Content-Security-Policy': {
                        'description': f"""**Vulnerability Details:**
The website is missing the {header} security header which helps prevent various client-side attacks including Cross-Site Scripting (XSS).

**Exploitation Method:**
1. Attacker can inject malicious scripts into the website
2. Users visiting the site may execute attacker-controlled code
3. Can lead to session hijacking, data theft, or malware distribution

**Proof of Concept:**
Without CSP, an attacker can:
```html
<script>alert('XSS')</script>
<img src=x onerror=alert('XSS')>
```

**Impact:**
- Session hijacking and cookie theft
- Keylogging and credential theft
- Cryptocurrency mining
- Website defacement""".strip(),
                        'remediation': f'Add {header} header: Content-Security-Policy: default-src \'self\'; script-src \'self\' \'unsafe-inline\'; style-src \'self\' \'unsafe-inline\''
                    },
                    'X-Frame-Options': {
                        'description': f"""**Vulnerability Details:**
Missing {header} allows attackers to embed this website in iframes on malicious sites (Clickjacking).

**Exploitation Method:**
1. Attacker creates a malicious page with transparent iframe
2. Overlays legitimate site with fake UI elements
3. Tricks users into clicking attacker-controlled actions

**Proof of Concept:**
```html
<iframe src="{url}" style="opacity: 0.1;"></iframe>
<div style="position: absolute; top: 100px; left: 50px;">
    <button>Click me (actually clicking iframe content)</button>
</div>
```

**Impact:**
- Unauthorized actions performed as victim
- Credential theft through fake login forms
- Likejacking and content manipulation""".strip(),
                        'remediation': f'Add {header} header: X-Frame-Options: SAMEORIGIN or DENY'
                    },
                    'Strict-Transport-Security': {
                        'description': f"""**Vulnerability Details:**
Missing {header} makes the site vulnerable to protocol downgrade attacks and MITM attacks.

**Exploitation Method:**
1. Attacker performs SSL stripping attack
2. Forces user connection over unencrypted HTTP
3. Intercepts and modifies traffic

**Impact:**
- Credential theft
- Session hijacking
- Data interception""".strip(),
                        'remediation': f'Add {header} header: Strict-Transport-Security: max-age=31536000; includeSubDomains'
                    }
                }

                detail = header_details.get(header, {'description': description, 'remediation': f'Add the {header} header to improve security'})

                self.findings.append({
                    'type': 'security_headers',
                    'category': 'Security Headers',
                    'title': f'Missing {header}',
                    'description': detail['description'],
                    'severity': 'low',
                    'cvss': '2.0',
                    'remediation': detail['remediation'],
                    'exploitation_method': f'{header} bypass attack',
                    'references': 'OWASP Security Headers Project'
                })

        return security_headers

    def crawl_website(self, base_url, max_pages):
        """Crawl website to find all pages"""
        urls_to_scan = [base_url]
        self.visited_urls.add(base_url)

        try:
            response = self.session.get(base_url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')

            # Extract links
            links = soup.find_all('a', href=True)
            for link in links:
                href = link['href']
                full_url = urljoin(base_url, href)

                # Only scan same domain
                if urlparse(full_url).netloc == urlparse(base_url).netloc:
                    if full_url not in self.visited_urls and len(urls_to_scan) < max_pages:
                        urls_to_scan.append(full_url)
                        self.visited_urls.add(full_url)

        except Exception as e:
            st.error(f"Error crawling website: {str(e)}")

        return urls_to_scan

    def scan_single_page(self, url):
        """Scan a single page for vulnerabilities"""
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')

            # Check for various vulnerabilities
            self.check_xss_vulnerabilities(soup, url)
            self.check_sql_injection(soup, url)
            self.check_csrf_vulnerabilities(soup, url)
            self.check_sensitive_data_exposure(soup, url)
            self.check_broken_authentication(soup, url)
            self.check_security_misconfiguration(soup, url)
            self.check_broken_access_control(soup, url)

        except Exception as e:
            st.error(f"Error scanning page {url}: {str(e)}")

    def check_xss_vulnerabilities(self, soup, url):
        """Check for XSS vulnerabilities"""
        forms = soup.find_all('form')

        for form in forms:
            action = form.get('action', '')
            method = form.get('method', 'get').lower()

            # Check if form has CSRF protection
            has_csrf_token = False
            inputs = form.find_all('input')
            for input_field in inputs:
                if input_field.get('type') == 'hidden' and ('csrf' in input_field.get('name', '').lower() or 'token' in input_field.get('name', '').lower()):
                    has_csrf_token = True
                    break

            if not has_csrf_token:
                self.findings.append({
                    'type': 'xss',
                    'category': 'Cross-Site Scripting (XSS)',
                    'title': 'Form without CSRF Protection',
                    'description': f"""
**URL:** {url}
**Form Action:** {action}
**Method:** {method}

**Vulnerability Details:**
This form lacks Cross-Site Request Forgery (CSRF) protection, making it vulnerable to attacks where malicious websites can force users to perform unwanted actions on this site while authenticated.

**Exploitation Method:**
1. Attacker creates a malicious webpage with hidden form fields
2. User is tricked into visiting the attacker's page while logged into the target site
3. Attacker's page automatically submits form to {action} with malicious data
4. Target site processes the request as if it came from the legitimate user

**Proof of Concept:**
```html
<!-- Attacker's malicious page -->
<form action="{urljoin(url, action)}" method="{method}" style="display:none;">
    <input type="hidden" name="malicious_field" value="attacker_controlled_data">
    <!-- Add other required fields -->
</form>
<script>document.forms[0].submit();</script>
```

**Impact:**
- Unauthorized actions performed as the victim user
- Data modification or deletion
- Account compromise
- Reputation damage

**References:**
- OWASP CSRF Prevention Cheat Sheet
- OWASP Top 10: Broken Access Control
                    """.strip(),
                    'severity': 'medium',
                    'cvss': '6.5',
                    'remediation': 'Add CSRF tokens to all forms. Implement same-site cookie attributes and origin header checks.',
                    'exploitation_method': 'CSRF attack via malicious webpage form submission',
                    'poc': 'Available in description above'
                })

    def check_sql_injection(self, soup, url):
        """Check for SQL injection vulnerabilities"""
        forms = soup.find_all('form')

        for form in forms:
            inputs = form.find_all('input')
            for input_field in inputs:
                input_name = input_field.get('name', '')
                input_type = input_field.get('type', '')

                # Look for suspicious input patterns
                if input_name and any(keyword in input_name.lower() for keyword in ['id', 'user', 'search', 'query']):
                    self.findings.append({
                        'type': 'sqli',
                        'category': 'SQL Injection',
                        'title': 'Potential SQL Injection Vector',
                        'description': f"""
**URL:** {url}
**Input Name:** {input_name}
**Input Type:** {input_type}

**Vulnerability Details:**
This input field appears to be used for database queries and may be vulnerable to SQL injection if user input is not properly sanitized or parameterized.

**Exploitation Method:**
1. Identify input fields that interact with databases (search, login, user input)
2. Test with malicious SQL payloads to bypass authentication or extract data
3. Use time-based or error-based techniques to extract database information

**Common SQL Injection Payloads:**
```sql
' OR '1'='1
' OR '1'='1' --
' UNION SELECT username, password FROM users --
'; DROP TABLE users; --
' AND (SELECT COUNT(*) FROM users) > 0 --
```

**Proof of Concept:**
```bash
# Basic test
curl -X POST "{url}" -d "{input_name}=test' OR '1'='1"

# Extract database version
curl -X POST "{url}" -d "{input_name}=test' UNION SELECT @@VERSION --"

# Time-based blind injection
curl -X POST "{url}" -d "{input_name}=test' AND (SELECT SLEEP(5)) --"
```

**Impact:**
- Unauthorized database access
- Data extraction (users, passwords, sensitive information)
- Database modification or deletion
- Server compromise through advanced techniques

**References:**
- OWASP SQL Injection Prevention Cheat Sheet
- OWASP Top 10: Injection
                        """.strip(),
                        'severity': 'high',
                        'cvss': '8.5',
                        'remediation': 'Use prepared statements, stored procedures, or ORM frameworks. Implement input validation and least privilege database accounts.',
                        'exploitation_method': 'SQL injection via malicious input payloads',
                        'poc': 'Available in description above'
                    })

    def check_csrf_vulnerabilities(self, soup, url):
        """Check for CSRF vulnerabilities"""
        # Already covered in XSS check, but add additional checks
        pass

    def check_sensitive_data_exposure(self, soup, url):
        """Check for sensitive data exposure"""
        # Check for email addresses in HTML
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, str(soup))

        if emails:
            self.findings.append({
                'type': 'info_leak',
                'category': 'Information Disclosure',
                'title': 'Email Addresses Exposed',
                'description': f"""
**URL:** {url}
**Exposed Emails:** {', '.join(set(emails))}

**Vulnerability Details:**
Email addresses are exposed in the HTML source code, making them easily harvestable by attackers and bots.

**Exploitation Method:**
1. Automated tools scan websites for email patterns using regex
2. Attackers build databases of email addresses for spam campaigns
3. Social engineering attacks target individuals using harvested emails
4. Email addresses used for account enumeration and password attacks

**Proof of Concept:**
```bash
# Simple email harvesting script
import requests
import re

response = requests.get('{url}')
emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{{2,}}\\b', response.text)
print('Found emails:', emails)
```

**Impact:**
- Spam and phishing campaigns targeting exposed emails
- Social engineering attacks using personal information
- Account enumeration for targeted attacks
- Reputation damage from spam complaints

**References:**
- OWASP Information Disclosure
- Personal Data Protection Guidelines
                """.strip(),
                'severity': 'low',
                'cvss': '2.0',
                'remediation': 'Remove email addresses from client-side code. Use contact forms or obfuscate emails with JavaScript or CSS.',
                'exploitation_method': 'Automated email harvesting and social engineering',
                'poc': 'Available in description above'
            })

        # Check for other sensitive patterns
        sensitive_patterns = {
            'Phone Numbers': r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',
            'Social Security Numbers': r'\b\d{3}-\d{2}-\d{4}\b',
            'Credit Card Numbers': r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b',
            'API Keys': r'\b[A-Za-z0-9]{32,}\b',  # Generic pattern for API keys
        }

        for pattern_name, pattern in sensitive_patterns.items():
            matches = re.findall(pattern, str(soup))
            if matches:
                self.findings.append({
                    'type': 'info_leak',
                    'category': 'Information Disclosure',
                    'title': f'{pattern_name} Exposed',
                    'description': f"""
**URL:** {url}
**Pattern:** {pattern_name}
**Found:** {len(matches)} potential matches

**Vulnerability Details:**
Potential {pattern_name.lower()} found in HTML source code.

**Impact:**
- Identity theft and fraud
- Unauthorized access to services
- Financial loss
- Privacy violations

**Remediation:**
Remove sensitive data from client-side code and implement proper data handling procedures.
                    """.strip(),
                    'severity': 'medium',
                    'cvss': '4.0',
                    'remediation': f'Remove {pattern_name.lower()} from client-side code. Store sensitive data server-side only.',
                    'exploitation_method': f'Automated harvesting of {pattern_name.lower()}',
                    'references': 'OWASP Sensitive Data Exposure'
                })

    def check_broken_authentication(self, soup, url):
        """Check for broken authentication"""
        # Look for weak password policies
        password_inputs = soup.find_all('input', {'type': 'password'})

        for pwd_input in password_inputs:
            minlength = pwd_input.get('minlength', '')
            pattern = pwd_input.get('pattern', '')

            if not minlength or int(minlength) < 8:
                self.findings.append({
                    'type': 'auth',
                    'category': 'Broken Authentication',
                    'title': 'Weak Password Policy',
                    'description': f"""
**URL:** {url}
**Minimum Length:** {minlength or 'Not set'}
**Pattern:** {pattern or 'Not set'}

Password field does not enforce strong password requirements.
                    """.strip(),
                    'severity': 'medium',
                    'cvss': '5.0',
                    'remediation': 'Enforce strong password policies (minimum 8 characters, complexity requirements)'
                })

    def check_security_misconfiguration(self, soup, url):
        """Check for security misconfiguration"""
        # Check for default pages
        default_files = ['admin.php', 'admin.aspx', 'login.php', 'config.php', 'backup.sql']
        current_path = urlparse(url).path

        for default_file in default_files:
            if default_file in current_path:
                self.findings.append({
                    'type': 'misconfig',
                    'category': 'Security Misconfiguration',
                    'title': 'Potential Default File Access',
                    'description': f"""
**URL:** {url}

Accessing what appears to be a default administrative or configuration file.
                    """.strip(),
                    'severity': 'medium',
                    'cvss': '5.0',
                    'remediation': 'Remove default files from production or restrict access with proper authentication'
                })

    def check_broken_access_control(self, soup, url):
        """Check for broken access control"""
        # Look for admin panels without authentication
        admin_keywords = ['admin', 'administrator', 'moderator', 'dashboard', 'control-panel']

        if any(keyword in url.lower() for keyword in admin_keywords):
            # Check if page requires authentication
            forms = soup.find_all('form')
            has_login_form = any('login' in str(form).lower() or 'password' in str(form).lower() for form in forms)

            if not has_login_form:
                self.findings.append({
                    'type': 'access_control',
                    'category': 'Broken Access Control',
                    'title': 'Administrative Page Without Authentication',
                    'description': f"""
**URL:** {url}

Administrative page accessible without apparent authentication requirements.
                    """.strip(),
                    'severity': 'high',
                    'cvss': '7.5',
                    'remediation': 'Implement proper authentication and authorization for administrative pages'
                })

    def generate_hackerone_report(self):
        """Generate HackerOne formatted report"""
        if not self.findings:
            st.success("‚úÖ No vulnerabilities found!")
            return

        # Group findings by severity
        severity_count = {'high': 0, 'medium': 0, 'low': 0, 'info': 0}
        for finding in self.findings:
            severity_count[finding['severity']] += 1

        # Create report
        report = {
            'title': f'Website Security Assessment - {len(self.findings)} findings',
            'summary': f"""
Comprehensive security scan completed with the following findings:
- High severity: {severity_count['high']}
- Medium severity: {severity_count['medium']}
- Low severity: {severity_count['low']}
- Informational: {severity_count['info']}

Scan performed on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            """.strip(),
            'findings': self.findings,
            'recommendations': self.generate_recommendations()
        }

        return report

    def generate_recommendations(self):
        """Generate general recommendations"""
        recommendations = [
            "Implement proper input validation and sanitization",
            "Use prepared statements for database queries",
            "Implement CSRF protection on all forms",
            "Use HTTPS for all communications",
            "Keep software and dependencies updated",
            "Implement proper session management",
            "Use security headers (CSP, HSTS, etc.)",
            "Regular security audits and penetration testing"
        ]

        return recommendations

def main():
    st.set_page_config(
        page_title="Website Vulnerability Scanner",
        page_icon="üîí",
        layout="wide"
    )

    st.title("üîí Website Vulnerability Scanner")
    st.markdown("### Comprehensive Bug Bounty Scanner for HackerOne")

    # Sidebar
    st.sidebar.header("Scan Configuration")
    max_pages = st.sidebar.slider("Max Pages to Scan", min_value=1, max_value=50, value=10)
    scan_forms = st.sidebar.checkbox("Scan Forms", value=True)
    scan_headers = st.sidebar.checkbox("Check Security Headers", value=True)

    # Main content
    url = st.text_input("Enter Website URL", placeholder="https://example.com")

    if st.button("üöÄ Start Scan", type="primary"):
        if not url:
            st.error("Please enter a URL to scan")
            return

        # Validate URL
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        # Initialize scanner
        scanner = WebsiteVulnerabilityScanner()

        # Progress bar
        progress_bar = st.progress(0)
        status_text = st.empty()

        # Perform scan
        with st.container():
            scanner.scan_website(url, max_pages)

        progress_bar.progress(100)
        status_text.text("‚úÖ Scan completed!")

        # Display results
        if scanner.findings:
            report = scanner.generate_hackerone_report()

            st.success(f"‚úÖ Scan completed! Found {len(scanner.findings)} findings")

            # Summary
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("High", len([f for f in scanner.findings if f['severity'] == 'high']))
            with col2:
                st.metric("Medium", len([f for f in scanner.findings if f['severity'] == 'medium']))
            with col3:
                st.metric("Low", len([f for f in scanner.findings if f['severity'] == 'low']))
            with col4:
                st.metric("Info", len([f for f in scanner.findings if f['severity'] == 'info']))

            # Detailed findings
            st.header("üìã Detailed Findings")

            for i, finding in enumerate(scanner.findings, 1):
                with st.expander(f"{i}. {finding['title']} ({finding['severity'].upper()})"):
                    st.markdown("**Category:** " + finding['category'])
                    st.markdown("**Description:**")
                    st.markdown(finding['description'])
                    st.markdown("**CVSS Score:** " + finding['cvss'])
                    st.markdown("**Remediation:**")
                    st.info(finding['remediation'])

            # HackerOne Export
            st.header("üìÑ HackerOne Report Format")

            report_text = f"""
# {report['title']}

## Summary
{report['summary']}

## Findings
"""

            for finding in report['findings']:
                report_text += f"""
### {finding['title']}
- **Category:** {finding['category']}
- **Severity:** {finding['severity'].upper()}
- **CVSS Score:** {finding['cvss']}

**Description:**
{finding['description']}

**Remediation:**
{finding['remediation']}

---
"""

            report_text += """
## General Recommendations
""" + "\n".join(f"- {rec}" for rec in report['recommendations'])

            st.download_button(
                label="üì• Download HackerOne Report",
                data=report_text,
                file_name=f"hackerone_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                mime="text/markdown"
            )

            st.text_area("Report Preview", report_text, height=300)
        else:
            st.success("‚úÖ No vulnerabilities found! The website appears to be secure.")

if __name__ == "__main__":
    main()
